# ã‚¿ã‚¹ã‚¯06: Claude APIé€£æº

## ğŸ“‹ æ¦‚è¦

| é …ç›® | å†…å®¹ |
|------|------|
| æ‹…å½“ | ğŸ¤– AI Agent |
| æ‰€è¦æ™‚é–“ | 1æ™‚é–“ |
| å‰ææ¡ä»¶ | ã‚¿ã‚¹ã‚¯03å®Œäº† |
| æˆæœç‰© | Claude ã‚µãƒ¼ãƒ“ã‚¹ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼ |

---

## ğŸ“ å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«

### backend/app/services/llm/base.py

```python
"""LLM åŸºåº•ã‚¯ãƒ©ã‚¹"""
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional


@dataclass
class LLMResponse:
    content: str
    model: str
    input_tokens: int
    output_tokens: int


@dataclass
class LLMConfig:
    model: str = "claude-sonnet-4-20250514"
    max_tokens: int = 8192
    temperature: float = 0.7


class BaseLLMService(ABC):
    @abstractmethod
    async def generate(self, system_prompt: str, user_prompt: str, config: Optional[LLMConfig] = None) -> LLMResponse:
        pass
```

### backend/app/services/llm/claude_service.py

```python
"""Claude API ã‚µãƒ¼ãƒ“ã‚¹"""
from typing import Optional
import anthropic
from tenacity import retry, stop_after_attempt, wait_exponential
from app.core.config import get_settings
from app.core.exceptions import ExternalServiceError
from app.services.llm.base import BaseLLMService, LLMConfig, LLMResponse

settings = get_settings()


class ClaudeService(BaseLLMService):
    def __init__(self):
        self.client = anthropic.AsyncAnthropic(api_key=settings.anthropic_api_key)
        self.default_config = LLMConfig()

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=30))
    async def generate(self, system_prompt: str, user_prompt: str, config: Optional[LLMConfig] = None) -> LLMResponse:
        cfg = config or self.default_config
        try:
            response = await self.client.messages.create(
                model=cfg.model, max_tokens=cfg.max_tokens, temperature=cfg.temperature,
                system=system_prompt, messages=[{"role": "user", "content": user_prompt}]
            )
            content = "".join(b.text for b in response.content if b.type == "text")
            return LLMResponse(content=content, model=response.model, input_tokens=response.usage.input_tokens, output_tokens=response.usage.output_tokens)
        except anthropic.APIError as e:
            raise ExternalServiceError("Claude API", str(e))


claude_service = ClaudeService()
```

### backend/app/services/prompts/prompt_builder.py

```python
"""ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼"""
import re
from dataclasses import dataclass
from typing import Any, Optional
from app.db.models import PromptTemplate


@dataclass
class BuiltPrompt:
    system_prompt: str
    user_prompt: str
    variables: dict[str, Any]


class PromptBuilder:
    DEFAULT_SYSTEM = """ã‚ãªãŸã¯SEOã«å¼·ã„Webãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚é«˜å“è³ªãªè¨˜äº‹ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
- è¦‹å‡ºã—ï¼ˆh2, h3ï¼‰ã‚’ä½¿ã£ã¦æ§‹é€ åŒ–
- å…·ä½“ä¾‹ã‚’äº¤ãˆã¦èª¬æ˜
- è‡ªç„¶ãªæ—¥æœ¬èªã§èª­ã¿ã‚„ã™ã"""

    DEFAULT_USER = """ã€Œ{keyword}ã€ã«ã¤ã„ã¦è¨˜äº‹ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
ã€è¦ä»¶ã€‘æ–‡å­—æ•°: {char_count_min}ã€œ{char_count_max}æ–‡å­—ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: Markdown
ã€æ§‹æˆã€‘ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆh1ï¼‰ã€å°å…¥ã€æœ¬æ–‡ï¼ˆh2/h3ï¼‰ã€ã¾ã¨ã‚"""

    def build(self, template: Optional[PromptTemplate], keyword: str, options: Optional[dict] = None) -> BuiltPrompt:
        variables = {"keyword": keyword, "char_count_min": 3000, "char_count_max": 4000}
        if options:
            variables.update(options)

        system = template.system_prompt if template else self.DEFAULT_SYSTEM
        user_tpl = template.user_prompt_template if template else self.DEFAULT_USER
        user = re.sub(r"\{(\w+)\}", lambda m: str(variables.get(m.group(1), m.group(0))), user_tpl)
        return BuiltPrompt(system_prompt=system, user_prompt=user, variables=variables)


prompt_builder = PromptBuilder()
```

### backend/app/services/prompts/response_parser.py

```python
"""ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‘ãƒ¼ã‚µãƒ¼"""
import re
from dataclasses import dataclass


@dataclass
class ParsedArticle:
    title: str
    content: str
    char_count: int
    is_valid: bool
    errors: list[str]


class ResponseParser:
    def parse(self, response: str, min_chars: int = 2000, max_chars: int = 6000) -> ParsedArticle:
        content = re.sub(r"^```(?:markdown)?\n?|```$", "", response, flags=re.MULTILINE).strip()
        title_match = re.search(r"^#\s+(.+)$", content, re.MULTILINE)
        title = title_match.group(1).strip() if title_match else ""

        plain = re.sub(r"^#+\s+", "", content, flags=re.MULTILINE)
        plain = re.sub(r"\*\*(.+?)\*\*", r"\1", plain)
        char_count = len(plain)

        errors = []
        if not title:
            errors.append("ã‚¿ã‚¤ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        if char_count < min_chars:
            errors.append(f"æ–‡å­—æ•°ä¸è¶³: {char_count}/{min_chars}")

        return ParsedArticle(title=title, content=content, char_count=char_count, is_valid=len(errors) == 0, errors=errors)


response_parser = ResponseParser()
```

---

## âœ… å®Œäº†æ¡ä»¶

```python
# Claude API ã®å‹•ä½œç¢ºèªï¼ˆPython REPLï¼‰
from app.services.llm.claude_service import claude_service
import asyncio

async def test():
    response = await claude_service.generate(
        "ã‚ãªãŸã¯å„ªç§€ãªãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™",
        "ã€ŒAIã€ã«ã¤ã„ã¦100æ–‡å­—ã§èª¬æ˜ã—ã¦ãã ã•ã„"
    )
    print(response.content)

asyncio.run(test())
```

---

## ğŸ“Œ æ¬¡ã®ã‚¿ã‚¹ã‚¯

ã‚¿ã‚¹ã‚¯06å®Œäº†å¾Œã€**ã‚¿ã‚¹ã‚¯07: è¨˜äº‹ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** ã«é€²ã‚“ã§ãã ã•ã„ã€‚
